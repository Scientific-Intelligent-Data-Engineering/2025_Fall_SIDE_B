import time
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix
import operator
from scipy.ndimage import shift

# --- (1. ë°ì´í„° ì´ë™ í—¬í¼ í•¨ìˆ˜ - ë™ì¼) ---
def shift_image(image, dx, dy):
    image = image.reshape((28, 28))
    shifted_image = shift(image, [dy, dx], cval=0, mode='constant') 
    return shifted_image.reshape([-1])

def run_mnist_knn_pca_variance_search_v2():
    print("MNIST ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    
    # --- (2. ë°ì´í„° ë¡œë“œ ë° ë¶„í•  - ë™ì¼) ---
    mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')
    X, y = mnist["data"], mnist["target"].astype(int)
    X_train, X_test = X[:60000], X[60000:]
    y_train, y_test = y[:60000], y[60000:]
    print(f"ì›ë³¸ í›ˆë ¨ ì„¸íŠ¸: {len(y_train)}ê°œ, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {len(y_test)}ê°œ")

    # --- (3. ë°ì´í„° ì¦ê°• - ë™ì¼) ---
    print("\në°ì´í„° ì¦ê°• ì‹œì‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    X_train_augmented = [X_train]
    y_train_augmented = [y_train]
    shifts = [(0, 1), (0, -1), (1, 0), (-1, 0)] 
    for dy, dx in shifts:
        print(f"  ... (dy={dy}, dx={dx}) ì´ë™ ì ìš© ì¤‘ ...")
        X_shifted = np.apply_along_axis(shift_image, axis=1, arr=X_train, dx=dx, dy=dy)
        X_train_augmented.append(X_shifted)
        y_train_augmented.append(y_train)
    X_train_augmented = np.vstack(X_train_augmented)
    y_train_augmented = np.concatenate(y_train_augmented)
    print(f"ì´ í›ˆë ¨ ì„¸íŠ¸ í¬ê¸°: {len(y_train_augmented)}ê°œ")

    # --- (4. ìŠ¤ì¼€ì¼ë§ - 30ë§Œê°œ ë°ì´í„°ë¡œ 1íšŒ ìˆ˜í–‰ - ë™ì¼) ---
    print("\në°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘... (30ë§Œê°œ, ë§¤ìš° ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤)")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_augmented)
    X_test_scaled = scaler.transform(X_test)
    print("ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ.")

    # --- (5. PCA ë¶„ì‚° ë¹„ìœ¨ íŠœë‹ ë£¨í”„) ---
    
    # â­ï¸â­ï¸â­ï¸ í…ŒìŠ¤íŠ¸í•  ë¶„ì‚° ë¹„ìœ¨ ë¦¬ìŠ¤íŠ¸ ë³€ê²½ â­ï¸â­ï¸â­ï¸
    variance_ratios = [0.60, 0.62, 0.64, 0.66, 0.68, 0.7] 
    
    # Kê°’ì€ ì´ì „ ìµœê³  ì„±ëŠ¥(k=6)ìœ¼ë¡œ ê³ ì • (ì‹œê°„ ì ˆì•½)
    fixed_k = 6 
    
    results = [] # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    print(f"\n--- PCA ë¶„ì‚° ë¹„ìœ¨ íŠœë‹ ì‹œì‘ (í›„ë³´: {variance_ratios}) ---")
    print(f"(Kê°’ì€ {fixed_k}ë¡œ ê³ ì •. ê° ë£¨í”„ëŠ” ë§¤ìš° ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤...)")

    # --- ì˜¤ë¥˜ ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ (ë™ì¼) ---
    best_accuracy_so_far = 0.0
    best_predictions = None # ìµœê³  ì„±ëŠ¥ì¼ ë•Œì˜ ì˜ˆì¸¡ ê²°ê³¼(y_pred)ë¥¼ ì €ì¥í•  ë³€ìˆ˜
    best_model_details = {} # ìµœê³  ì„±ëŠ¥ì¼ ë•Œì˜ ì„¤ì •ì„ ì €ì¥í•  ë³€ìˆ˜

    for var_ratio in variance_ratios:
        print(f"\n[ ë¶„ì‚° ë¹„ìœ¨ = {var_ratio} ] í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # 1. PCA ì ìš© (ë£¨í”„ë§ˆë‹¤ ìƒˆë¡œ fit_transform)
        print(f"  PCA ì ìš© (n_components={var_ratio})...")
        pca_start_time = time.time()
        
        pca = PCA(n_components=var_ratio, random_state=42) 
        X_train_reduced = pca.fit_transform(X_train_scaled)
        X_test_reduced = pca.transform(X_test_scaled) 
        
        pca_time = time.time() - pca_start_time
        n_components_found = pca.n_components_ # 85% ë³´ì¡´ì— ëª‡ ê°œì˜ ì°¨ì›ì´ ì“°ì˜€ëŠ”ì§€ í™•ì¸
        
        print(f"  PCA ì™„ë£Œ (ì†Œìš” ì‹œê°„: {pca_time:.2f}ì´ˆ).")
        print(f"  >>> {var_ratio*100}% ë¶„ì‚° ë³´ì¡´ì— í•„ìš”í•œ ì°¨ì› ìˆ˜: {n_components_found}ê°œ <<<")

        # 2. KNN í›ˆë ¨ ë° ì˜ˆì¸¡
        print(f"  k={fixed_k} ëª¨ë¸ í›ˆë ¨/ì˜ˆì¸¡ ì¤‘...")
        knn_start_time = time.time()
        
        knn_clf = KNeighborsClassifier(n_neighbors=fixed_k, weights='distance', n_jobs=-1)
        knn_clf.fit(X_train_reduced, y_train_augmented)
        y_pred = knn_clf.predict(X_test_reduced) 
        
        knn_time = time.time() - knn_start_time
        print(f"  KNN ì™„ë£Œ (ì†Œìš” ì‹œê°„: {knn_time:.2f}ì´ˆ)")
        
        # 3. ê²°ê³¼ ì €ì¥ ë° 'ìµœê³  ëª¨ë¸' ê°±ì‹ 
        accuracy = accuracy_score(y_test, y_pred)
        print(f"  >>> ë¶„ì‚°={var_ratio} (ì°¨ì›={n_components_found}) ì¼ ë•Œ ì •í™•ë„: {accuracy * 100:.2f}% <<<")
        
        current_result = {
            'variance': var_ratio, 
            'n_components': n_components_found, 
            'accuracy': accuracy
        }
        results.append(current_result)
        
        if accuracy > best_accuracy_so_far:
            print(f"  *** ìƒˆ ìµœê³  ì •í™•ë„ ë‹¬ì„±! ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ***")
            best_accuracy_so_far = accuracy
            best_predictions = y_pred  
            best_model_details = current_result

    # --- (6. ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° ì˜¤ë¥˜ ë¶„ì„ - ë™ì¼) ---
    best_result_summary = max(results, key=lambda x: x['accuracy'])
    print("--- ëª¨ë“  ê²°ê³¼ ---")
    for res in results:
        print(f"ë¶„ì‚°: {res['variance']}, ì°¨ì›: {res['n_components']}, ì •í™•ë„: {res['accuracy'] * 100:.2f}%")
    print("\n--- ê°€ì¥ ì¢‹ì€ ê²°ê³¼ (ìš”ì•½) ---")
    print(f"ìµœì  ë¶„ì‚° ë¹„ìœ¨: {best_result_summary['variance']}, ì°¨ì› ìˆ˜: {best_result_summary['n_components']}, ìµœê³  ì •í™•ë„: {best_result_summary['accuracy'] * 100:.2f}%")

    print("\n\n--- â­ï¸ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì˜¤ë¥˜ ë¶„ì„ ---")
    print(f"(ë¶„ì„ ëŒ€ìƒ: ë¶„ì‚°={best_model_details['variance']}, ì°¨ì›={best_model_details['n_components']}, ì •í™•ë„={best_model_details['accuracy'] * 100:.2f}%)")

    if best_predictions is None:
        print("ì˜¤ë¥˜: ì˜ˆì¸¡ ê²°ê³¼ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        conf_mx = confusion_matrix(y_test, best_predictions)
        row_sums = conf_mx.sum(axis=1) 
        correct_predictions = np.diag(conf_mx)
        error_counts = row_sums - correct_predictions
        error_rates = error_counts / row_sums
        
        print("\n--- ğŸ“ˆ ìˆ«ìë³„ ì˜¤ë¥˜ìœ¨ (ì˜¤ë¥˜ê°€ ë§ì€ ìˆœ) ---")
        sorted_error_indices = np.argsort(error_rates)[::-1] 
        for idx in sorted_error_indices:
            print(f"ìˆ«ì {idx}: \t ì˜¤ë¥˜ìœ¨ {error_rates[idx] * 100:.2f}% \t (ì´ {row_sums[idx]}ê°œ ì¤‘ {error_counts[idx]}ê°œ ì˜¤ë¥˜)")

        np.fill_diagonal(conf_mx, 0) 
        max_error_count = np.max(conf_mx)
        max_error_indices = np.unravel_index(np.argmax(conf_mx), conf_mx.shape)
        actual_label = max_error_indices[0]
        predicted_label = max_error_indices[1]
        
        print(f"\n--- âš¡ï¸ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•œ ë‹¨ì¼ ì˜¤ë¥˜ ---")
        print(f"ì‹¤ì œ '{actual_label}'ì„(ë¥¼) '{predicted_label}'(ìœ¼)ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²½ìš°ê°€ {max_error_count}íšŒë¡œ ê°€ì¥ ë§ì•˜ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_mnist_knn_pca_variance_search_v2()

'''
k=6 fixed
--- ëª¨ë“  ê²°ê³¼ ---
ë¶„ì‚°: 0.60, ì°¨ì›: 68, ì •í™•ë„: 97.12%
ë¶„ì‚°: 0.62, ì°¨ì›: 74, ì •í™•ë„: 97.16%
ë¶„ì‚°: 0.64, ì°¨ì›: 81, ì •í™•ë„: 97.11%
ë¶„ì‚°: 0.66, ì°¨ì›: 88, ì •í™•ë„: 97.08%
ë¶„ì‚°: 0.68, ì°¨ì›: 96, ì •í™•ë„: 97.18%
ë¶„ì‚°: 0.70, ì°¨ì›: 104, ì •í™•ë„: 97.14%
ë¶„ì‚°: 0.75, ì°¨ì›: 127, ì •í™•ë„: 97.23% best
ë¶„ì‚°: 0.76, ì°¨ì›: 133, ì •í™•ë„: 97.19%
ë¶„ì‚°: 0.77, ì°¨ì›: 139, ì •í™•ë„: 97.17%
ë¶„ì‚°: 0.78, ì°¨ì›: 145, ì •í™•ë„: 97.18%
ë¶„ì‚°: 0.79, ì°¨ì›: 151, ì •í™•ë„: 97.16%
ë¶„ì‚°: 0.80, ì°¨ì›: 158, ì •í™•ë„: 97.13%
ë¶„ì‚°: 0.85, ì°¨ì›: 197, ì •í™•ë„: 97.08%        
ë¶„ì‚°: 0.86, ì°¨ì›: 206, ì •í™•ë„: 97.01%        
ë¶„ì‚°: 0.87, ì°¨ì›: 217, ì •í™•ë„: 96.94%        
ë¶„ì‚°: 0.88, ì°¨ì›: 228, ì •í™•ë„: 96.96%        
ë¶„ì‚°: 0.89, ì°¨ì›: 240, ì •í™•ë„: 96.90%        
ë¶„ì‚°: 0.90, ì°¨ì›: 253, ì •í™•ë„: 96.95%
ë¶„ì‚°: 0.93, ì°¨ì›: 305, ì •í™•ë„: 96.59%        
ë¶„ì‚°: 0.94, ì°¨ì›: 328, ì •í™•ë„: 96.52%        
ë¶„ì‚°: 0.95, ì°¨ì›: 356, ì •í™•ë„: 96.45%        
ë¶„ì‚°: 0.96, ì°¨ì›: 390, ì •í™•ë„: 96.40%        
ë¶„ì‚°: 0.97, ì°¨ì›: 433, ì •í™•ë„: 96.40%        
ë¶„ì‚°: 0.98, ì°¨ì›: 493, ì •í™•ë„: 96.33% 
'''